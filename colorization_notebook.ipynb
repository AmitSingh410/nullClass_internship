{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a43e4083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import DataLoader,random_split\n",
    "import torchvision\n",
    "from utils import (\n",
    "    CustomImageNet100Dataset as ImageNetColorizationDataset,\n",
    "    UNet32,\n",
    "    rgb_to_lab,\n",
    "    rgb_to_gray_with_clahe,\n",
    "    lab_to_rgb_torch,\n",
    "    train_model,\n",
    "    evaluate_model,\n",
    "    targeted_colorization_with_segmentation,\n",
    "    sam_targeted_colorization\n",
    ")\n",
    "import os \n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36abcdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a461091",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_dir = \"./data/imagenet100\"\n",
    "\n",
    "# Load train+val split from ImageNet\n",
    "full_train_ds = ImageNetColorizationDataset(root_dir=root_dir, train=True, augment=True, img_size=224)\n",
    "train_len = int(0.9 * len(full_train_ds))\n",
    "val_len = len(full_train_ds) - train_len\n",
    "train_ds, val_ds = random_split(full_train_ds, [train_len, val_len])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Load validation (test) set\n",
    "test_ds = ImageNetColorizationDataset(root_dir=root_dir, train=False, augment=False, img_size=224)\n",
    "test_loader = DataLoader(test_ds, batch_size=8, shuffle=False, num_workers=0, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3419d6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amit Singh\\AppData\\Local\\Temp\\ipykernel_42792\\2963291294.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_cifar_unet_clahe_lab.pth\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Model\n",
    "\n",
    "model=UNet32(base_ch=64).to(device)\n",
    "model.load_state_dict(torch.load(\"best_cifar_unet_clahe_lab.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4208712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 6641 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 1745 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 297 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 882 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 750 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 1023 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 247 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 203 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 476 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 13279 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 285 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 335 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 149 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 6881 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 526 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 5317 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 108 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 213 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 89 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 443 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 17470 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 2457 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 40799 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 4063 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 399 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 2038 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 5695 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 118 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 2790 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 7972 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 295 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 985 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 196 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 282 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 253 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 7221 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 104 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 6066 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 3558 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 430 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 11199 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 7045 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 33217 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 681 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 2979 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 7857 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n",
      "c:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:82: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 275 negative Z values that have been clipped to zero\n",
      "  rgb_img = color.lab2rgb(lab_img.astype(np.float64))   # [H,W,3] in [0,1]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbest_imagenet_unet_clahe_lab.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m75\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:458\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, device, save_path, epochs, patience)\u001b[39m\n\u001b[32m    455\u001b[39m model.train()\n\u001b[32m    456\u001b[39m running_train = \u001b[32m0.0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgray\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgray\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mL_gt\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mL_gt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:245\u001b[39m, in \u001b[36mCustomImageNet100Dataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m    244\u001b[39m     img_path, label = \u001b[38;5;28mself\u001b[39m.samples[idx]\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m     image = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    246\u001b[39m     image = \u001b[38;5;28mself\u001b[39m.transform(image)\n\u001b[32m    247\u001b[39m     gray = transforms.functional.rgb_to_grayscale(image)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\.venv\\Lib\\site-packages\\PIL\\Image.py:3505\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3502\u001b[39m     filename = os.fspath(fp)\n\u001b[32m   3504\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[32m-> \u001b[39m\u001b[32m3505\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3506\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    save_path=\"best_imagenet_unet_clahe_lab.pth\",\n",
    "    epochs=75,\n",
    "    patience=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2335209",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rgb_to_lab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:553\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, test_loader, device)\u001b[39m\n\u001b[32m    551\u001b[39m model.eval()\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrayscale_images\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgray\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpred_ab\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrayscale_images\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amit Singh\\Documents\\Projects\\nullClass_training-main\\utils.py:241\u001b[39m, in \u001b[36mCustomImageNet100Dataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    239\u001b[39m image = \u001b[38;5;28mself\u001b[39m.transform(image)\n\u001b[32m    240\u001b[39m gray = transforms.functional.rgb_to_grayscale(image)\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m lab = \u001b[43mrgb_to_lab\u001b[49m(image)\n\u001b[32m    242\u001b[39m L, ab = lab[\u001b[32m0\u001b[39m:\u001b[32m1\u001b[39m], lab[\u001b[32m1\u001b[39m:]\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    245\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mgray\u001b[39m\u001b[33m\"\u001b[39m: gray,\n\u001b[32m    246\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mL_gt\u001b[39m\u001b[33m\"\u001b[39m: L,\n\u001b[32m    247\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mab_gt\u001b[39m\u001b[33m\"\u001b[39m: ab\n\u001b[32m    248\u001b[39m }\n",
      "\u001b[31mNameError\u001b[39m: name 'rgb_to_lab' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e3ba2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
